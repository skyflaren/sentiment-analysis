{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SKLearn SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skyflaren/sentiment-analysis/blob/master/SKLearn_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1tqubJNdgNu",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyVGJTr3hou4",
        "colab_type": "text"
      },
      "source": [
        "An AI model to predict tweet sentiments, implementing a Gaussian distribution of Bayes' algorithm. \n",
        "\n",
        "Created by @[astrocat879](https://github.com/astrocat879), @[dulldesk](https://github.com/dulldesk), @[hewmatt10](https://github.com/hewmatt10), and @[skyflaren](https://github.com/skyflaren)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FanLqK8bllE",
        "colab_type": "text"
      },
      "source": [
        "# Preliminary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CfSbyGKhyDA",
        "colab_type": "text"
      },
      "source": [
        "Some preliminary work prior to training and predicting the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDzt2LulHyXT",
        "colab_type": "text"
      },
      "source": [
        "## Fetch Data\n",
        "Files uploaded to Google Colab are recycled after a certain number of hours. To get around this, our data is hosted remotely and as necessary we pull it into the environment using the `wget` module. These files are exact copies of the provided datasets in the [Division Sigma resources folder](https://drive.google.com/drive/folders/1EH9S0XcSlDqEG9f-gzOjk0We5hb5OvAq). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXhNxFrwZj0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wget\n",
        "import wget\n",
        "import os\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-zigpVZdhIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# location of the training data\n",
        "data_root = \"/content/given_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpADzXPVZ2TA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir(data_root): os.mkdir(data_root)\n",
        " \n",
        "download_root = \"https://skyflaren.github.io/sentiment-analysis/\"\n",
        "to_download = ['training_data.csv','contestant_judgment.csv']\n",
        "\n",
        "for filename in to_download:\n",
        "    # if dataset file already exists, remove it\n",
        "    if os.path.isfile(download_root+filename): \n",
        "        os.remove(download_root+filename)\n",
        "    wget.download(download_root+filename,data_root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg8Jq37IGUIV",
        "colab_type": "text"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYHDSP8PIEzd",
        "colab_type": "text"
      },
      "source": [
        "Here, we import all of the modules necessary to run the notebook. Their usages are as follows:\n",
        "\n",
        "### **textblob**\n",
        "\n",
        "Text analysis library used to perform common NLP tasks, such as lemmatization.\n",
        "\n",
        "### **pandas**\n",
        "\n",
        "Data analysis library used to easily manipulate and iterate through the training and judging dataset. \n",
        "\n",
        "### **re**\n",
        "\n",
        "Used for cleaning text input from the dataset via regular expressions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhdOaxR_uOfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob, Word\n",
        "import pandas as pd\n",
        "import re, random\n",
        "########################\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB,BernoulliNB, GaussianNB\n",
        "# from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL1x17G_UfDe",
        "colab_type": "text"
      },
      "source": [
        "### **NLTK**\n",
        "\n",
        "This library deals with natural language processing. We use the library to lemmatize words, and to classify them using the Naive Bayes algorithm. An overview of the downloads:\n",
        "\n",
        "#### **punkt**\n",
        "\n",
        "Sentence tokenizer\n",
        "\n",
        "#### **averaged_perceptron_tagger**\n",
        "\n",
        "Used for tagging words with their parts of speech\n",
        "\n",
        "#### **wordnet**\n",
        "\n",
        "Database of words\n",
        "\n",
        "#### **stopwords**\n",
        "\n",
        "Provides a list of common words, such as `between` or `the`, to be ignored as they do not provide value in tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiUtjuMcpSQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m textblob.download_corpora\n",
        "\n",
        "from nltk import download as nltk_download\n",
        "nltk_download('punkt')\n",
        "nltk_download('averaged_perceptron_tagger')\n",
        "nltk_download('wordnet')\n",
        "nltk_download('stopwords')\n",
        "\n",
        "from nltk import classify, NaiveBayesClassifier\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjCV-Pr_Oyv3",
        "colab_type": "text"
      },
      "source": [
        "# The Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSn_qNUbMqsI",
        "colab_type": "text"
      },
      "source": [
        "We first preprocess our data, cleaning the dataset, prior to training on it and judging on the judgement dataset. \n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDPSFYg6p2aZ",
        "colab_type": "text"
      },
      "source": [
        "Here, we use `pandas` to retrieve the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wClpVXmUJyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(data_root+'/training_data.csv', header=None)[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n25clh0_qr1n",
        "colab_type": "text"
      },
      "source": [
        "We use the `tags` attribute of the `TextBlob` object, which yields the part of speech tag for each word in the object. NLTK (and assumingly, TextBlob, as it is based off of NLTK) uses the [Penn Treebank parts of speech tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). \n",
        "\n",
        "To ensure that words such as `go` and `went` are grouped together, we lemmatized each word in the TextBlob before processing them. TextBlob's `lemmatize()` function uses WordNet ([source](https://textblob.readthedocs.io/en/dev/api_reference.html#textblob.blob.Word.lemmatize)), whose morphological processor uses [these parts of speech tags](https://wordnet.princeton.edu/documentation/morph3wn):\n",
        "\n",
        "| Part of Speech | POS Short Form used in `lemmatize()` |\n",
        "| :----:  | :-----: |\n",
        "| Noun | n |\n",
        "| Verb | v |\n",
        "| Adjective | a |\n",
        "| Adverb | r | \n",
        "| Adjective satellite | s | \n",
        "\n",
        "This dictionary maps the NLTK/Penn POS tags (used by `TextBlob.tags`) to their corrsponding WordNet POS tag (used by `TextBlob.lemmatize()`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG2wUCZfpXfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parts_of_speech = {'nn':'n','nns':'n','nnp':'n','nnps':'n','prp':'n','prp$':'n','vb':'v','vbg':'v','vbd':'v','vbn':'v','vbp':'v','vbz':'v','rb':'r','rbr':'r','rbs':'r','wrb':'r','jj':'a','jjr':'a','jjs':'a'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOZLMv2krVR3",
        "colab_type": "text"
      },
      "source": [
        "`delim` - a custom regex-based deliminator is used to properly tokenize contractions and keep them intact (e.g. `can't` instead of `can` and `'t`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTg338dHp6_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "delim = RegexpTokenizer(\"[\\\\w']+|[^\\\\w\\\\s]+\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj6yUApAsIsC",
        "colab_type": "text"
      },
      "source": [
        "`dataset` - a list of tuples containing a map (key is a lemmatized word, value is `True`) and whether it should be a `1` or `0`\n",
        "\n",
        "`sentiment_ind`, `text_ind` - constants for the column numbers of the input text and expected output sentiment value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpI0Ys_Jpy_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# == Initialize variables\n",
        "sentiment_ind = 3\n",
        "text_ind = 2\n",
        "dataset = []\n",
        "all_text = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-0EmFwsJIDc",
        "colab_type": "text"
      },
      "source": [
        "`stop_words` - a set of the stop words to be ignored in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCzwt9pK8Gds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQc4HosUHBj6",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElYCnwqfNw5c",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roY4oRQ2JALS",
        "colab_type": "text"
      },
      "source": [
        "Here, we iterate and clean all words in a text, adding them to the `dataset`. This process will take about 25 minutes.\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va3IXhxYTHO_",
        "colab_type": "text"
      },
      "source": [
        "`cleanse()` - given a pair of a word and its parts-of-speech tag (Penn guidelines), the function returns a cleaned, lemmatized version of the word. If the cleaned word is an empty string, then `None` is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBQaktO3WAh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# == Cleaning (function)\n",
        "\n",
        "def cleanse(word):\n",
        "    # Remove symbols and ignore numbers\n",
        "    word_txt = re.sub('[.,$^!@#%&*\"()~`\\\\-_+=\\\\[\\\\]{}<>/\\\\\\\\:;]*','',word[0]).strip().lower()\n",
        "\n",
        "    # Ignore stop words, empty strings, other users, or numbers\n",
        "    if word_txt == '' or word_txt in stop_words or word[0].strip()[0] == '@' or  re.sub(\"[0-9]\\s*\",'',word_txt) == '':\n",
        "        return None\n",
        "\n",
        "    w = Word(word_txt)\n",
        "    # Return the lemmatized word if it can be lemmatized; if not, return the original word\n",
        "    return w.lemmatize( str(parts_of_speech[word[1]].lower()) ) if word[1] in parts_of_speech else word_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9LwuwnhaYY0",
        "colab_type": "text"
      },
      "source": [
        "Here, the text and corresponding sentiment value in each entry in the original dataset is processed. After stripping the text feature of any hyperlinks, the text is broken down into its words and parts of speech. Each word is cleaned of unnecessary characters and/or discarded if the word does not present value (e.g. a stray symbol). \n",
        "\n",
        "`cleaned_words` is added to the `dataset`, which is eventually used by the classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NwH8iZAVWrfx",
        "colab": {}
      },
      "source": [
        "# == Preprocessing\n",
        "\n",
        "for ind, row in data.iterrows():\n",
        "    # The text feature in the current netry\n",
        "    txt = row[text_ind]\n",
        "\n",
        "    # Dictionary of cleaned words used by the training classifiers\n",
        "    cleaned_words = {}\n",
        "\n",
        "    # An integer representing the label for the current feature\n",
        "    pos_or_neg = int(row[sentiment_ind])\n",
        "\n",
        "    # Remove hyperlinks\n",
        "    txt = re.sub('https?:\\\\/\\\\/[^\\\\s]*(?=(\\\\s|$))','',txt)\n",
        "\n",
        "    # Breaks down each sentence into (word, part of speech tag) pairs\n",
        "    sentence = TextBlob(txt, tokenizer=delim) \n",
        "\n",
        "    # Iterate through all (word, part of speech tag) pairs in the sentence, cleanse them, and add them\n",
        "    for word in sentence.tags:\n",
        "        ret = cleanse(word)\n",
        "        if ret == None: continue\n",
        "\n",
        "        cleaned_words[ret] = True\n",
        "\n",
        "    # Append the sentence's worth of cleaned words\n",
        "    dataset.append( tuple([cleaned_words, pos_or_neg]) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ais9XgmTNvFn",
        "colab_type": "text"
      },
      "source": [
        "### Naive Bayes\n",
        "\n",
        "Naive Bayes assigns values to words based on training data, then judges the sentiment of a sentence by using those values. Below is the fundamental Bayes theorem.\n",
        "\n",
        "$$P(A|B) = \\frac{P(B|A) * P(A)}{P(B)}$$\n",
        "\n",
        "Bayes theorem helps us find the probability of an event occuring given another even occurs based off of other dependent and independent events. We use the probability that a word is in one sentiment to find the probability that given the sentiment, the word occurs. This will help us determine a relative probability that a piece of text is a sentiment, which will ultimately tell us which sentiment the piece of text is more likely to be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnJkYtp81ORC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# == Training\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "data_len = len(dataset)\n",
        "\n",
        "data_for_training = dataset[int(0.7*data_len):]\n",
        "data_for_judging = dataset[:int(0.7*data_len)]\n",
        "\n",
        "classifier = NaiveBayesClassifier.train(data_for_training)\n",
        "print(\"Accuracy is:\", classify.accuracy(classifier, data_for_judging))\n",
        "\n",
        "print(classifier.show_most_informative_features(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAuuJ6mggfTS",
        "colab_type": "text"
      },
      "source": [
        "## Predicting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JB_OefFgmce",
        "colab_type": "text"
      },
      "source": [
        "Here, the model predicts the judgement dataset's sentiments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jtomlbv8dLBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "judgedata = pd.read_csv(data_root+'/contestant_judgment.csv', header=None)[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QC5dO3LhJN7",
        "colab_type": "text"
      },
      "source": [
        "The text cleaning process used in the training section is performed on the judgement data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey6CDiJBsojQ",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title\n",
        "# == Predicting\n",
        "predictions = []\n",
        "\n",
        "for ind, row in judgedata.iterrows():\n",
        "    txt = row[text_ind]\n",
        "    cleaned_words = {}\n",
        "\n",
        "    # Breaks down each sentence into (word, part of speech tag) pairs\n",
        "    sentence = TextBlob(txt, tokenizer=delim) \n",
        "\n",
        "    # Iterate through all (word, part of speech tag) pairs in the sentence, cleanse them, and add them\n",
        "    for word in sentence.tags:\n",
        "        ret = cleanse(word)\n",
        "        if ret != None:\n",
        "            cleaned_words[ret] = True\n",
        "\n",
        "    predictions.append(classifier.classify(cleaned_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlUbtAysgh6I",
        "colab_type": "text"
      },
      "source": [
        "Finally, we build a dataframe of the predicted data and export it to a csv. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsGi5ithhBTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_df = pd.DataFrame(columns=[\"ID\",\"User\",\"Text\",\"Sentiment\"])\n",
        "\n",
        "for ind,row in judgedata.iterrows():\n",
        "    # Rename the columns in the row from integer labels to string ones. The integer-labelled columns are then droppde. \n",
        "    row['ID'] = row[0]\n",
        "    row['User'] = row[1]\n",
        "    row['Text'] = row[2]\n",
        "    row['Sentiment'] = int(predictions[ind] == 1)\n",
        "\n",
        "    row = row.drop([0,1,2])\n",
        "    \n",
        "    # Add this formatted entry with its sentiment prediction to the dataframe\n",
        "    prediction_df = prediction_df.append(row,ignore_index=True)\n",
        "\n",
        "# Export the dataframe to the csv to be submitted\n",
        "prediction_df.to_csv(dataroot+\"/predictions.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}